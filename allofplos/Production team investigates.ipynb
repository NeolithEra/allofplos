{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plos_corpus import *\n",
    "from samples.corpus_analysis import *\n",
    "corpusdir_prod = '../../allofplos/allofplos/allofplos_xml/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q: Are annotation DOIs resolving correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_annotation_dict(save_output=True):\n",
    "    \"\"\"\n",
    "    For every article file whose DOI contains the word \"annotation\", check whether its DOI resolves correctly\n",
    "    by creating a dictionary of the resolution status.\n",
    "    :return: dictionary where each key is a DOI, each value is associated resolution of that DOI via doi.org.\n",
    "    :param save_output: exports dictionary to csv\n",
    "    \"\"\"\n",
    "    dois = [file_to_doi(file) for file in listdir_nohidden(corpusdir)]\n",
    "    annotation_list = [x for x in dois if x.startswith('10.1371/annotation')]\n",
    "    anno_dict = {doi: check_if_doi_resolves(doi) for doi in annotation_list}\n",
    "    \n",
    "    if save_output:\n",
    "        with open('annotations.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['DOI', 'Resolution'])\n",
    "            for key, value in anno_dict.items():\n",
    "                writer.writerow([key, value])\n",
    "\n",
    "    return anno_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run this\n",
    "make_annotation_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Q: Which `<contrib>` elements follow a certain pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_tina_test_set():\n",
    "    \"\"\"\n",
    "    Return a list of DOIs good for Tina's function\n",
    "    \"\"\"\n",
    "    random_list_of_dois = get_random_list_of_dois(count=10)\n",
    "    random_list_of_articles = [doi_to_path(doi) for doi in random_list_of_dois if 'annotation' not in doi]\n",
    "    search_1_dois = ('10.1371/journal.pmed.1002035', '10.1371/journal.pone.0047559', '10.1371/journal.pone.0047944')\n",
    "    search_1_articles = [doi_to_path(doi) for doi in search_1_dois]\n",
    "    search_test_set = list(set(random_list_of_articles + search_1_articles))\n",
    "    return search_test_set\n",
    "\n",
    "def find_contrib_pattern(article_list=None, csv=True):\n",
    "    \"\"\"\n",
    "    Three separate searches would be most helpful:\n",
    "    Search #1: Find all articles where a <contrib> element contains an <on-behalf-of> element. \n",
    "       Example: pmed.1002035, pone.0047559, and pone.0047944 should all be found by this search.\n",
    "    Search #2: Find all articles where a <contrib> element that contains an <on-behalf-of> element is\n",
    "    immediately followed by <contrib> element that contains a <collab> element.\n",
    "       Example: pone.0047559 and pone.0047944 should both be found by this search, but not pmed.1002035.\n",
    "    Search #3: Find all articles where a <contrib> element that contains an <on-behalf-of> element is\n",
    "    immediately followed by <contrib> element that contains a <collab> element that contains a <contrib-group>.\n",
    "       Example: pone.0047944 should be found by this search, but not pmed.1002035 or pone.0047559.)\n",
    "    To test this function, use get_tina_test_set() to run on a subset of articles\n",
    "    \"\"\"\n",
    "    if article_list is None:\n",
    "        article_list = listdir_nohidden(corpusdir)\n",
    "\n",
    "    search_1_results = []\n",
    "    search_2_results = []\n",
    "    search_3_results = []\n",
    "\n",
    "    for article_file in article_list:\n",
    "        tag_path_elements = ('/',\n",
    "                             'article',\n",
    "                             'front',\n",
    "                             'article-meta')\n",
    "        article_xml = get_articleXML_content(article_file, tag_path_elements=tag_path_elements)\n",
    "        meta_categories = article_xml[0].getchildren()\n",
    "        contrib_groups = [category for category in meta_categories if category.tag == 'contrib-group']\n",
    "        for contrib_group in contrib_groups:\n",
    "            for contributor in contrib_group:\n",
    "                for element in contributor:\n",
    "                    if element.tag == 'on-behalf-of':\n",
    "                        search_1_results.append(filename_to_doi(article_file))\n",
    "                        next_element = contributor.getnext()\n",
    "                        if next_element is not None:\n",
    "                            for elem in next_element:\n",
    "                                if elem.tag == 'collab':\n",
    "                                    search_2_results.append(filename_to_doi(article_file))\n",
    "                                    for subelem in elem:\n",
    "                                        if subelem.tag == 'contrib-group':\n",
    "                                            search_3_results.append(filename_to_doi(article_file))\n",
    "                                            break\n",
    "\n",
    "    search_1_results = set(search_1_results)\n",
    "    search_2_results = set(search_2_results)\n",
    "    search_3_results = set(search_3_results)\n",
    "    search_results = list(set(search_1_results + search_2_results + search_3_results))\n",
    "    doi_results = []\n",
    "    for doi in search_results:\n",
    "        if doi in search_1_results:\n",
    "            s1 = 'yes'\n",
    "        else:\n",
    "            s1 = 'no'\n",
    "        if doi in search_2_results:\n",
    "            s2 = 'yes'\n",
    "        else:\n",
    "            s2 = 'no'\n",
    "        if doi in search_3_results:\n",
    "            s3 = 'yes'\n",
    "        else:\n",
    "            s3 = 'no'\n",
    "        doi_result = (doi, s1, s2, s3)\n",
    "        doi_results.append(doi_result)\n",
    "    if csv:\n",
    "        with open('search_results.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['DOI', 'Search 1', 'Search 2', 'Search 3'])\n",
    "            for doi_result in sorted(doi_results):\n",
    "                writer.writerow(doi_result)\n",
    "    return doi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_random_list_of_dois' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-53b4176d4446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test this function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tina_test_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdoi_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_contrib_pattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-3f86500faf78>\u001b[0m in \u001b[0;36mget_tina_test_set\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mDOIs\u001b[0m \u001b[0mgood\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mTina\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrandom_list_of_dois\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_list_of_dois\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mrandom_list_of_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoi_to_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom_list_of_dois\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'annotation'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msearch_1_dois\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'10.1371/journal.pmed.1002035'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'10.1371/journal.pone.0047559'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'10.1371/journal.pone.0047944'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_random_list_of_dois' is not defined"
     ]
    }
   ],
   "source": [
    "# test this function\n",
    "test_list = get_tina_test_set()\n",
    "doi_results = find_contrib_pattern(article_list=test_list, csv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(doi_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run this function for real\n",
    "doi_results = find_contrib_pattern()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q: Which articles after 2015 have 2 or more corrections attached?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corrections_article_list, corrected_article_list = get_corrected_article_list()\n",
    "multiple_corrections = set([article for article in corrected_article_list\n",
    "                            if corrected_article_list.count(article) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "multiple_corrections.remove('10.1371/journal.')\n",
    "multiple_corrections_post_2015 = [article for article in multiple_corrections\n",
    "                                  if get_article_pubdate(doi_to_file(article)).year >= 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "multiple_corrections_post_2015\n",
    "with open('2_or_more_corrections.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['DOI'])\n",
    "    for item in multiple_corrections_post_2015:\n",
    "        writer.writerow(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q: Which articles have a series of table-wrap graphic elements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "example_doi = '10.1371/journal.pone.0068090'\n",
    "search_1_file = 'xml_testing/Search-1_TRUE.xml'\n",
    "search_2_file = 'xml_testing/Search-2_TRUE.xml'\n",
    "intro_file = doi_to_path(example_doi, directory=corpusdir_prod)\n",
    "search_1_fail_list = []\n",
    "fail_file = doi_to_path('10.1371/journal.pone.0183466', directory=corpusdir_prod)\n",
    "test_list = [fail_file, intro_file, search_1_file, search_2_file]\n",
    "\n",
    "def find_table_wraps(article):\n",
    "    \"\"\"\n",
    "    find all articles with a `table-wrap` element. of those, if there is no immediate sub-tag of\n",
    "    'alternative' in table\n",
    "    \"\"\"\n",
    "    intro_condition = False\n",
    "    intro_condition_overall = False\n",
    "    search1_ids = []\n",
    "    search2_ids = []\n",
    "    alternative_graphic_ids = []\n",
    "\n",
    "    article_tree = et.parse(article, parser=et.XMLParser(remove_comments=True))  # exclude commented-out tables\n",
    "    table_wraps = article_tree.findall('.//table-wrap')\n",
    "    if table_wraps:\n",
    "        for table_wrap in table_wraps:\n",
    "            table_parts = table_wrap.getchildren()\n",
    "            # intro condition 1: table-wrap element does not include a direct child of <alternatives><graphic>\n",
    "            alternatives_parts = [table_part for table_part in table_parts if 'alternatives' in table_part.tag]\n",
    "            if not alternatives_parts:\n",
    "                intro_condition_1 = True\n",
    "            else:\n",
    "                for table_part in alternatives_parts:\n",
    "                    table_subparts = table_part.getchildren()\n",
    "                    if all('graphic' not in table_subpart.tag for table_subpart in table_subparts):\n",
    "                        intro_condition_1 = True\n",
    "                    else:\n",
    "                        intro_condition_1 = False\n",
    "                        new_alternative_graphic_ids = [table_subpart.attrib['id'] for table_subpart in table_subparts if 'graphic' in table_subpart.tag]\n",
    "                        alternative_graphic_ids.extend(new_alternative_graphic_ids)\n",
    "\n",
    "            # intro condition 2: table-wrap element does not include a direct child of <graphic>\n",
    "            if all('graphic' not in table_part.tag for table_part in table_parts):\n",
    "                intro_condition_2 = True\n",
    "            else:\n",
    "                intro_condition_2 = False\n",
    "            \n",
    "            if intro_condition_1 and intro_condition_2:\n",
    "                intro_condition = True\n",
    "                # keep track of articles that have any table match intro condition\n",
    "                intro_condition_overall = True\n",
    "\n",
    "            if intro_condition:\n",
    "                graphics = table_wrap.findall('.//graphic')\n",
    "                if graphics:\n",
    "                    new_search1_ids = [graphic.attrib['id'] for graphic in graphics]\n",
    "                    search1_ids.extend(new_search1_ids)\n",
    "                inline_graphics = table_wrap.findall('.//inline-graphic')\n",
    "                if inline_graphics:\n",
    "                    try:\n",
    "                        search2_ids = [inline.attrib['id'] for inline in inline_graphics]\n",
    "                    except KeyError:\n",
    "                        print('{} has search 2 results but no ids: {}'.format(article, inline_graphics))\n",
    "                        search2_ids = [inline.attrib for inline in inline_graphics]\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    else:\n",
    "        return intro_condition_overall, bool(search1_ids), bool(search2_ids)\n",
    "\n",
    "    if search1_ids and alternative_graphic_ids:\n",
    "        # exclude graphics elements that are already accounted for under an <alternatives> tag\n",
    "        search1_ids = [did for did in search1_ids if did not in alternative_graphic_ids]\n",
    "    if not search1_ids:\n",
    "        search1_ids = False\n",
    "    elif len(search1_ids) == 1:\n",
    "        search1_ids = search1_ids[0]\n",
    "    if not search2_ids:\n",
    "        search2_ids = False\n",
    "    elif len(search2_ids) == 1:\n",
    "        search2_ids = search2_ids[0]\n",
    "    return intro_condition_overall, search1_ids, search2_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../allofplos/allofplos/allofplos_xml/journal.pone.0183466.xml False False False\n",
      "../../allofplos/allofplos/allofplos_xml/journal.pone.0068090.xml True False False\n",
      "xml_testing/Search-1_TRUE.xml True pmed.1002397.e001g False\n",
      "xml_testing/Search-2_TRUE.xml True False pmed.1002397.e001g\n"
     ]
    }
   ],
   "source": [
    "# testing the code\n",
    "for article_file in test_list:\n",
    "    intro_condition, search1_ids, search2_ids = find_table_wraps(article_file)\n",
    "    print(article_file, intro_condition, search1_ids, search2_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221852\n",
      "['../../allofplos/allofplos/allofplos_xml/journal.ppat.1000896.xml', '../../allofplos/allofplos/allofplos_xml/journal.pone.0065590.xml', '../../allofplos/allofplos/allofplos_xml/journal.pone.0036030.xml', '../../allofplos/allofplos/allofplos_xml/journal.pone.0026652.xml', '../../allofplos/allofplos/allofplos_xml/journal.pone.0029438.xml', '../../allofplos/allofplos/allofplos_xml/journal.pgen.1000989.xml', '../../allofplos/allofplos/allofplos_xml/journal.pone.0089988.xml', '../../allofplos/allofplos/allofplos_xml/journal.pone.0015594.xml', '../../allofplos/allofplos/allofplos_xml/journal.pone.0149634.xml', '../../allofplos/allofplos/allofplos_xml/journal.pone.0000707.xml']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12% ( 27042 of 221852) |#############                                                                                              | Elapsed Time: 0:04:33 ETA: 0:57:29"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../allofplos/allofplos/allofplos_xml/journal.pone.0002468.xml has search 2 results but no ids: [<Element inline-graphic at 0x115a65248>, <Element inline-graphic at 0x115a65d88>, <Element inline-graphic at 0x115a654c8>, <Element inline-graphic at 0x115a65208>, <Element inline-graphic at 0x115a65108>]\n",
      "../../allofplos/allofplos/allofplos_xml/journal.pone.0002468.xml has search 2 results but no ids: [<Element inline-graphic at 0x115a65dc8>, <Element inline-graphic at 0x115a65308>, <Element inline-graphic at 0x115a65cc8>, <Element inline-graphic at 0x115a65f48>, <Element inline-graphic at 0x115a653c8>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52% (116850 of 221852) |########################################################                                                   | Elapsed Time: 0:19:57 ETA: 0:18:13"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../allofplos/allofplos/allofplos_xml/journal.pone.0075851.xml has search 2 results but no ids: [<Element inline-graphic at 0x11559cc88>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (221852 of 221852) |##########################################################################################################| Elapsed Time: 0:38:14 Time: 0:38:14\n"
     ]
    }
   ],
   "source": [
    "# running over entire corpus, randomized, with a progressbar\n",
    "import progressbar\n",
    "from random import shuffle\n",
    "\n",
    "table_results = []\n",
    "file_list = listdir_nohidden(corpusdir_prod)\n",
    "shuffle(file_list)\n",
    "\n",
    "bar = progressbar.ProgressBar(redirect_stdout=True, max_value=len(file_list))\n",
    "for i, article_file in enumerate(file_list):\n",
    "    intro_condition, search1_ids, search2_ids = find_table_wraps(article_file)\n",
    "    if intro_condition:\n",
    "        result = [filename_to_doi(article_file), search1_ids, search2_ids]\n",
    "        table_results.append(result)\n",
    "    bar.update(i+1)\n",
    "bar.finish()\n",
    "\n",
    "# print(table_results)\n",
    "with open('table_graphics_search_results.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['DOI', 'Search 1', 'Search 2'])\n",
    "    for doi_result in sorted(table_results):\n",
    "        writer.writerow(doi_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Which Aperta articles have a group collaboration contributor element?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Example: 10.1371/journal.pmed.1002170\n",
    "<contrib-group>\n",
    "<contrib contrib-type=\"author\" xlink:type=\"simple\">\n",
    "<collab>International Ebola Response Team</collab>\n",
    "<xref ref-type=\"fn\" rid=\"fn001\">\n",
    "<sup>¶</sup>\n",
    "\n",
    "<fn fn-type=\"other\" id=\"fn001\">\n",
    "<p>\n",
    "¶ The International Ebola Response Team comprises the authors listed in this article in alphabetical order\n",
    "</p>\n",
    "</fn>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    " def get_article_collab(doi, corpusdir=corpusdir_prod):\n",
    "    \"\"\"\n",
    "    For a given PLOS article, see if there is a collaborator group in the authors list. Print data if so\n",
    "    :return: tuple of doi, collaborators, and the footnote number if so\n",
    "    \"\"\"\n",
    "    tag_path_elements = ('/',\n",
    "                         'article',\n",
    "                         'front',\n",
    "                         'article-meta')\n",
    "    article_xml = get_article_xml(doi_to_file(doi, directory=corpusdir), tag_path_elements=tag_path_elements)\n",
    "    meta_categories = article_xml[0].getchildren()\n",
    "    contrib_groups = [category for category in meta_categories if category.tag == 'contrib-group']\n",
    "    collab = False\n",
    "    rid = ''\n",
    "    footnote = False\n",
    "    collab_tuple = ''\n",
    "    try:\n",
    "        for contrib_group in contrib_groups:\n",
    "            for contrib in contrib_group:\n",
    "                if contrib.attrib['contrib-type'] == 'author':\n",
    "                    for child in contrib:\n",
    "                        if child.tag == \"collab\":\n",
    "                            collab = True\n",
    "                            collaborators = child.text\n",
    "                            continue\n",
    "                        if child.tag == 'role':\n",
    "                            continue\n",
    "                        elif child.tag == 'xref':\n",
    "                            rid = (child.attrib['rid'])\n",
    "                        if collab and rid:\n",
    "                            break\n",
    "\n",
    "    except IndexError:\n",
    "        print('No authors found for {}'.format(doi))\n",
    "        return False\n",
    "\n",
    "    if collab and rid:\n",
    "        tag_path_elements = ('/',\n",
    "                             'article',\n",
    "                             'front',\n",
    "                             'article-meta',\n",
    "                             'author-notes')\n",
    "\n",
    "        article_xml = get_article_xml(doi_to_file(doi, directory=corpusdir), tag_path_elements=tag_path_elements)\n",
    "        notes = article_xml[0].getchildren()\n",
    "        for note in notes:\n",
    "            if note.tag == 'fn' and rid in note.attrib.values():\n",
    "                footnote = True\n",
    "        if footnote is False:\n",
    "            print('footnote not found for {}'.format(doi))\n",
    "\n",
    "        collab_tuple = (doi, collaborators, rid)\n",
    "\n",
    "    elif collab:\n",
    "        print('rid not found for {}'.format(doi))\n",
    "\n",
    "    if collab_tuple:\n",
    "        print(collab_tuple)\n",
    "\n",
    "    return collab_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Restrict to PLOS Biology Aperta articles\n",
    "article_list = [article for article in listdir_nohidden(corpusdir_prod) if 'pbio.2' in article] \n",
    "doi_list = [file_to_doi(article) for article in article_list]\n",
    "doi_list.append('10.1371/journal.pmed.1002170')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10.1371/journal.pbio.2001069', 'CycliX consortium', 'fn001')\n",
      "('10.1371/journal.pbio.2001855', 'BEEHIVE collaboration', 'fn001')\n",
      "('10.1371/journal.pmed.1002170', 'International Ebola Response Team', 'fn001')\n"
     ]
    }
   ],
   "source": [
    "for doi in doi_list:\n",
    "    get_article_collab(doi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
