{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plos_corpus import *\n",
    "from samples.corpus_analysis import *\n",
    "corpusdir_prod = '../../allofplos/allofplos/allofplos_xml/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Are annotation DOIs resolving correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_annotation_dict(save_output=True):\n",
    "    \"\"\"\n",
    "    For every article file whose DOI contains the word \"annotation\", check whether its DOI resolves correctly\n",
    "    by creating a dictionary of the resolution status.\n",
    "    :return: dictionary where each key is a DOI, each value is associated resolution of that DOI via doi.org.\n",
    "    :param save_output: exports dictionary to csv\n",
    "    \"\"\"\n",
    "    dois = [file_to_doi(file) for file in listdir_nohidden(corpusdir)]\n",
    "    annotation_list = [x for x in dois if x.startswith('10.1371/annotation')]\n",
    "    anno_dict = {doi: check_if_doi_resolves(doi) for doi in annotation_list}\n",
    "    \n",
    "    if save_output:\n",
    "        with open('annotations.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['DOI', 'Resolution'])\n",
    "            for key, value in anno_dict.items():\n",
    "                writer.writerow([key, value])\n",
    "\n",
    "    return anno_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this\n",
    "make_annotation_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Q: Which `<contrib>` elements follow a certain pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tina_test_set():\n",
    "    \"\"\"\n",
    "    Return a list of DOIs good for Tina's function\n",
    "    \"\"\"\n",
    "    random_list_of_dois = get_random_list_of_dois(count=10)\n",
    "    random_list_of_articles = [doi_to_file(doi) for doi in random_list_of_dois if 'annotation' not in doi]\n",
    "    search_1_dois = ('10.1371/journal.pmed.1002035', '10.1371/journal.pone.0047559', '10.1371/journal.pone.0047944')\n",
    "    search_1_articles = [doi_to_file(doi) for doi in search_1_dois]\n",
    "    search_test_set = list(set(random_list_of_articles + search_1_articles))\n",
    "    return search_test_set\n",
    "\n",
    "def find_contrib_pattern(article_list=None, csv=True):\n",
    "    \"\"\"\n",
    "    Three separate searches would be most helpful:\n",
    "    Search #1: Find all articles where a <contrib> element contains an <on-behalf-of> element. \n",
    "       Example: pmed.1002035, pone.0047559, and pone.0047944 should all be found by this search.\n",
    "    Search #2: Find all articles where a <contrib> element that contains an <on-behalf-of> element is\n",
    "    immediately followed by <contrib> element that contains a <collab> element.\n",
    "       Example: pone.0047559 and pone.0047944 should both be found by this search, but not pmed.1002035.\n",
    "    Search #3: Find all articles where a <contrib> element that contains an <on-behalf-of> element is\n",
    "    immediately followed by <contrib> element that contains a <collab> element that contains a <contrib-group>.\n",
    "       Example: pone.0047944 should be found by this search, but not pmed.1002035 or pone.0047559.)\n",
    "    To test this function, use get_tina_test_set() to run on a subset of articles\n",
    "    \"\"\"\n",
    "    if article_list is None:\n",
    "        article_list = listdir_nohidden(corpusdir)\n",
    "\n",
    "    search_1_results = []\n",
    "    search_2_results = []\n",
    "    search_3_results = []\n",
    "\n",
    "    for article_file in article_list:\n",
    "        tag_path_elements = ('/',\n",
    "                             'article',\n",
    "                             'front',\n",
    "                             'article-meta')\n",
    "        article_xml = get_articleXML_content(article_file, tag_path_elements=tag_path_elements)\n",
    "        meta_categories = article_xml[0].getchildren()\n",
    "        contrib_groups = [category for category in meta_categories if category.tag == 'contrib-group']\n",
    "        for contrib_group in contrib_groups:\n",
    "            for contributor in contrib_group:\n",
    "                for element in contributor:\n",
    "                    if element.tag == 'on-behalf-of':\n",
    "                        search_1_results.append(file_to_doi(article_file))\n",
    "                        next_element = contributor.getnext()\n",
    "                        if next_element is not None:\n",
    "                            for elem in next_element:\n",
    "                                if elem.tag == 'collab':\n",
    "                                    search_2_results.append(file_to_doi(article_file))\n",
    "                                    for subelem in elem:\n",
    "                                        if subelem.tag == 'contrib-group':\n",
    "                                            search_3_results.append(file_to_doi(article_file))\n",
    "                                            break\n",
    "\n",
    "    search_1_results = set(search_1_results)\n",
    "    search_2_results = set(search_2_results)\n",
    "    search_3_results = set(search_3_results)\n",
    "    search_results = list(set(search_1_results + search_2_results + search_3_results))\n",
    "    doi_results = []\n",
    "    for doi in search_results:\n",
    "        if doi in search_1_results:\n",
    "            s1 = 'yes'\n",
    "        else:\n",
    "            s1 = 'no'\n",
    "        if doi in search_2_results:\n",
    "            s2 = 'yes'\n",
    "        else:\n",
    "            s2 = 'no'\n",
    "        if doi in search_3_results:\n",
    "            s3 = 'yes'\n",
    "        else:\n",
    "            s3 = 'no'\n",
    "        doi_result = (doi, s1, s2, s3)\n",
    "        doi_results.append(doi_result)\n",
    "    if csv:\n",
    "        with open('search_results.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['DOI', 'Search 1', 'Search 2', 'Search 3'])\n",
    "            for doi_result in sorted(doi_results):\n",
    "                writer.writerow(doi_result)\n",
    "    return doi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test this function\n",
    "test_list = get_tina_test_set()\n",
    "doi_results = find_contrib_pattern(article_list=test_list, csv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(doi_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this function for real\n",
    "doi_results = find_contrib_pattern()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Which articles after 2015 have 2 or more corrections attached?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrections_article_list, corrected_article_list = get_corrected_article_list()\n",
    "multiple_corrections = set([article for article in corrected_article_list\n",
    "                            if corrected_article_list.count(article) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiple_corrections.remove('10.1371/journal.')\n",
    "multiple_corrections_post_2015 = [article for article in multiple_corrections\n",
    "                                  if get_article_pubdate(doi_to_file(article)).year >= 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiple_corrections_post_2015\n",
    "with open('2_or_more_corrections.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['DOI'])\n",
    "    for item in multiple_corrections_post_2015:\n",
    "        writer.writerow(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Which articles have a series of table-wrap elements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_doi = '10.1371/journal.pone.0068090'\n",
    "search_1_file = 'xml_testing/Search-1_TRUE.xml'\n",
    "search_2_file = 'xml_testing/Search-2_TRUE.xml'\n",
    "intro_file = doi_to_file(example_doi)\n",
    "fail_file = doi_to_file('10.1371/journal.pone.0182980')\n",
    "test_list = [intro_file, search_1_file, search_2_file, fail_file]\n",
    "\n",
    "intro_condition = []\n",
    "search_1 = []\n",
    "search_2 = []\n",
    "\n",
    "def find_table_wraps(article):\n",
    "    \"\"\"\n",
    "    find all articles with a `table-wrap` element. of those, if there is no immediate sub-tag of\n",
    "    'alternative' in table\n",
    "    \"\"\"\n",
    "    intro_condition = False\n",
    "    search_1 = False\n",
    "    search_2 = False\n",
    "\n",
    "    article_tree = et.parse(article)\n",
    "    table_wraps = article_tree.findall('.//table-wrap')\n",
    "    if table_wraps:\n",
    "        for table_wrap in table_wraps:\n",
    "            try:\n",
    "                if all('alternatives' not in table_part.tag for table_part in table_wrap) and \\\n",
    "                   all('graphic' not in table_part.tag for table_part in table_wrap):\n",
    "                    intro_condition = True\n",
    "            except TypeError:\n",
    "                # this is an imperfect work-around. if alternatives were a sub-sub-element,\n",
    "                # it would be incorrectly excluded from intro_\n",
    "                alternatives = table_wrap.findall('.//alternatives')\n",
    "                if alternatives == 0:\n",
    "                    intro_condition = True\n",
    "            if intro_condition:\n",
    "                danger = table_wrap.findall('.//graphic')\n",
    "                if danger:\n",
    "                    search_1 = True\n",
    "                danger2 = table_wrap.findall('.//inline-graphic')\n",
    "                if danger2:\n",
    "                    search_2 = True\n",
    "            else:\n",
    "                pass\n",
    "                       \n",
    "#                 for table_part in table_parts:\n",
    "#                     if 'alternatives' in table_part.tag:\n",
    "#                         print('alternatives')\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return intro_condition, search_1, search_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allofplos_xml/journal.pone.0068090.xml True False False\n",
      "xml_testing/Search-1_TRUE.xml True True False\n",
      "xml_testing/Search-2_TRUE.xml True True True\n",
      "allofplos_xml/journal.pone.0182980.xml False False False\n"
     ]
    }
   ],
   "source": [
    "table_results = []\n",
    "for article_file in test_list:\n",
    "    intro_condition, search_1, search_2 = find_table_wraps(article_file)\n",
    "    print(article_file, intro_condition, search_1, search_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table_results = []\n",
    "file_list = listdir_nohidden(corpusdir)\n",
    "for article_file in file_list:\n",
    "    intro_condition, search_1, search_2 = find_table_wraps(article_file)\n",
    "    if intro_condition:\n",
    "        result = [file_to_doi(article_file), search_1, search_2]\n",
    "        table_results.append(result)\n",
    "\n",
    "# print(table_results)\n",
    "with open('table_search_results_revised.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['DOI', 'Search 1', 'Search 2'])\n",
    "    for doi_result in sorted(table_results):\n",
    "        writer.writerow(doi_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for article_file in listdir_nohidden(corpusdir)[180000:180010]:\n",
    "    print(find_table_wraps(article_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Which Aperta articles have a group collaboration contributor element?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: 10.1371/journal.pmed.1002170\n",
    "<contrib-group>\n",
    "<contrib contrib-type=\"author\" xlink:type=\"simple\">\n",
    "<collab>International Ebola Response Team</collab>\n",
    "<xref ref-type=\"fn\" rid=\"fn001\">\n",
    "<sup>¶</sup>\n",
    "\n",
    "<fn fn-type=\"other\" id=\"fn001\">\n",
    "<p>\n",
    "¶ The International Ebola Response Team comprises the authors listed in this article in alphabetical order\n",
    "</p>\n",
    "</fn>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def get_article_collab(doi, corpusdir=corpusdir_prod):\n",
    "    \"\"\"\n",
    "    For a given PLOS article, see if there is a collaborator group in the authors list. Print data if so\n",
    "    :return: tuple of doi, collaborators, and the footnote number if so\n",
    "    \"\"\"\n",
    "    tag_path_elements = ('/',\n",
    "                         'article',\n",
    "                         'front',\n",
    "                         'article-meta')\n",
    "    article_xml = get_article_xml(doi_to_file(doi, directory=corpusdir), tag_path_elements=tag_path_elements)\n",
    "    meta_categories = article_xml[0].getchildren()\n",
    "    contrib_groups = [category for category in meta_categories if category.tag == 'contrib-group']\n",
    "    collab = False\n",
    "    rid = ''\n",
    "    footnote = False\n",
    "    collab_tuple = ''\n",
    "    try:\n",
    "        for contrib_group in contrib_groups:\n",
    "            for contrib in contrib_group:\n",
    "                if contrib.attrib['contrib-type'] == 'author':\n",
    "                    for child in contrib:\n",
    "                        if child.tag == \"collab\":\n",
    "                            collab = True\n",
    "                            collaborators = child.text\n",
    "                            continue\n",
    "                        if child.tag == 'role':\n",
    "                            continue\n",
    "                        elif child.tag == 'xref':\n",
    "                            rid = (child.attrib['rid'])\n",
    "                        if collab and rid:\n",
    "                            break\n",
    "\n",
    "    except IndexError:\n",
    "        print('No authors found for {}'.format(doi))\n",
    "        return False\n",
    "\n",
    "    if collab and rid:\n",
    "        tag_path_elements = ('/',\n",
    "                             'article',\n",
    "                             'front',\n",
    "                             'article-meta',\n",
    "                             'author-notes')\n",
    "\n",
    "        article_xml = get_article_xml(doi_to_file(doi, directory=corpusdir), tag_path_elements=tag_path_elements)\n",
    "        notes = article_xml[0].getchildren()\n",
    "        for note in notes:\n",
    "            if note.tag == 'fn' and rid in note.attrib.values():\n",
    "                footnote = True\n",
    "        if footnote is False:\n",
    "            print('footnote not found for {}'.format(doi))\n",
    "\n",
    "        collab_tuple = (doi, collaborators, rid)\n",
    "\n",
    "    elif collab:\n",
    "        print('rid not found for {}'.format(doi))\n",
    "\n",
    "    if collab_tuple:\n",
    "        print(collab_tuple)\n",
    "\n",
    "    return collab_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Restrict to PLOS Biology Aperta articles\n",
    "article_list = [article for article in listdir_nohidden(corpusdir_prod) if 'pbio.2' in article] \n",
    "doi_list = [file_to_doi(article) for article in article_list]\n",
    "doi_list.append('10.1371/journal.pmed.1002170')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10.1371/journal.pbio.2001069', 'CycliX consortium', 'fn001')\n",
      "('10.1371/journal.pbio.2001855', 'BEEHIVE collaboration', 'fn001')\n",
      "('10.1371/journal.pmed.1002170', 'International Ebola Response Team', 'fn001')\n"
     ]
    }
   ],
   "source": [
    "for doi in doi_list:\n",
    "    get_article_collab(doi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
