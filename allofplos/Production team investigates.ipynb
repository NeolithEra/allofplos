{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plos_corpus import *\n",
    "from samples.corpus_analysis import *\n",
    "corpusdir = 'allofplos_xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Are annotation DOIs resolving correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotation_dict(save_output=True):\n",
    "    \"\"\"\n",
    "    For every article file whose DOI contains the word \"annotation\", check whether its DOI resolves correctly\n",
    "    by creating a dictionary of the resolution status.\n",
    "    :return: dictionary where each key is a DOI, each value is associated resolution of that DOI via doi.org.\n",
    "    :param save_output: exports dictionary to csv\n",
    "    \"\"\"\n",
    "    dois = [file_to_doi(file) for file in listdir_nohidden(corpusdir)]\n",
    "    annotation_list = [x for x in dois if x.startswith('10.1371/annotation')]\n",
    "    anno_dict = {doi: check_if_doi_resolves(doi) for doi in annotation_list}\n",
    "    \n",
    "    if save_output:\n",
    "        with open('annotations.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['DOI', 'Resolution'])\n",
    "            for key, value in anno_dict.items():\n",
    "                writer.writerow([key, value])\n",
    "\n",
    "    return anno_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this\n",
    "make_annotation_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Q: Which `<contrib>` elements follow a certain pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tina_test_set():\n",
    "    \"\"\"\n",
    "    Return a list of DOIs good for Tina's function\n",
    "    \"\"\"\n",
    "    random_list_of_dois = get_random_list_of_dois(count=10)\n",
    "    random_list_of_articles = [doi_to_file(doi) for doi in random_list_of_dois if 'annotation' not in doi]\n",
    "    search_1_dois = ('10.1371/journal.pmed.1002035', '10.1371/journal.pone.0047559', '10.1371/journal.pone.0047944')\n",
    "    search_1_articles = [doi_to_file(doi) for doi in search_1_dois]\n",
    "    search_test_set = list(set(random_list_of_articles + search_1_articles))\n",
    "    return search_test_set\n",
    "\n",
    "def find_contrib_pattern(article_list=None, csv=True):\n",
    "    \"\"\"\n",
    "    Three separate searches would be most helpful:\n",
    "    Search #1: Find all articles where a <contrib> element contains an <on-behalf-of> element. \n",
    "       Example: pmed.1002035, pone.0047559, and pone.0047944 should all be found by this search.\n",
    "    Search #2: Find all articles where a <contrib> element that contains an <on-behalf-of> element is\n",
    "    immediately followed by <contrib> element that contains a <collab> element.\n",
    "       Example: pone.0047559 and pone.0047944 should both be found by this search, but not pmed.1002035.\n",
    "    Search #3: Find all articles where a <contrib> element that contains an <on-behalf-of> element is\n",
    "    immediately followed by <contrib> element that contains a <collab> element that contains a <contrib-group>.\n",
    "       Example: pone.0047944 should be found by this search, but not pmed.1002035 or pone.0047559.)\n",
    "    To test this function, use get_tina_test_set() to run on a subset of articles\n",
    "    \"\"\"\n",
    "    if article_list is None:\n",
    "        article_list = listdir_nohidden(corpusdir)\n",
    "\n",
    "    search_1_results = []\n",
    "    search_2_results = []\n",
    "    search_3_results = []\n",
    "\n",
    "    for article_file in article_list:\n",
    "        tag_path_elements = ('/',\n",
    "                             'article',\n",
    "                             'front',\n",
    "                             'article-meta')\n",
    "        article_xml = get_articleXML_content(article_file, tag_path_elements=tag_path_elements)\n",
    "        meta_categories = article_xml[0].getchildren()\n",
    "        contrib_groups = [category for category in meta_categories if category.tag == 'contrib-group']\n",
    "        for contrib_group in contrib_groups:\n",
    "            for contributor in contrib_group:\n",
    "                for element in contributor:\n",
    "                    if element.tag == 'on-behalf-of':\n",
    "                        search_1_results.append(file_to_doi(article_file))\n",
    "                        next_element = contributor.getnext()\n",
    "                        if next_element is not None:\n",
    "                            for elem in next_element:\n",
    "                                if elem.tag == 'collab':\n",
    "                                    search_2_results.append(file_to_doi(article_file))\n",
    "                                    for subelem in elem:\n",
    "                                        if subelem.tag == 'contrib-group':\n",
    "                                            search_3_results.append(file_to_doi(article_file))\n",
    "                                            break\n",
    "\n",
    "    search_1_results = set(search_1_results)\n",
    "    search_2_results = set(search_2_results)\n",
    "    search_3_results = set(search_3_results)\n",
    "    search_results = list(set(search_1_results + search_2_results + search_3_results))\n",
    "    doi_results = []\n",
    "    for doi in search_results:\n",
    "        if doi in search_1_results:\n",
    "            s1 = 'yes'\n",
    "        else:\n",
    "            s1 = 'no'\n",
    "        if doi in search_2_results:\n",
    "            s2 = 'yes'\n",
    "        else:\n",
    "            s2 = 'no'\n",
    "        if doi in search_3_results:\n",
    "            s3 = 'yes'\n",
    "        else:\n",
    "            s3 = 'no'\n",
    "        doi_result = (doi, s1, s2, s3)\n",
    "        doi_results.append(doi_result)\n",
    "    if csv:\n",
    "        with open('search_results.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['DOI', 'Search 1', 'Search 2', 'Search 3'])\n",
    "            for doi_result in sorted(doi_results):\n",
    "                writer.writerow(doi_result)\n",
    "    return doi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test this function\n",
    "test_list = get_tina_test_set()\n",
    "doi_results = find_contrib_pattern(article_list=test_list, csv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doi_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this function for real\n",
    "doi_results = find_contrib_pattern()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Which articles after 2015 have 2 or more corrections attached?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections_article_list, corrected_article_list = get_corrected_article_list()\n",
    "multiple_corrections = set([article for article in corrected_article_list\n",
    "                            if corrected_article_list.count(article) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_corrections.remove('10.1371/journal.')\n",
    "multiple_corrections_post_2015 = [article for article in multiple_corrections\n",
    "                                  if get_article_pubdate(doi_to_file(article)).year >= 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_corrections_post_2015\n",
    "with open('2_or_more_corrections.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['DOI'])\n",
    "    for item in multiple_corrections_post_2015:\n",
    "        writer.writerow(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
